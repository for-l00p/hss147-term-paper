{"status":"OK","result":{"originalLocale":"en","allowViewHistory":false,"creationTimeSeconds":1405345601,"rating":4,"authorHandle":"IvayloS","modificationTimeSeconds":1405345743,"id":13047,"title":"\u003cp\u003eAdaptive problem scores\u003c/p\u003e","locale":"en","content":"\u003cdiv class\u003d\"ttypography\"\u003e\u003cp\u003eI have noticed that problem score distributions are often unfair and the last Round(FF) in Div 1 is not an exception. Although second and third problem gave the same number of points, third problem was solved by way less competitors.\u003c/p\u003e\u003cp\u003eI have a proposal on how to address this: introduce adaptive problem scoring. The problem score will be determined by the number of successful submissions for this problem. There are two options for this: either have a set rule of the type \u0026quot;If problem A has between X% and Y% accepted submissions it will be a 1000 pointer\u0026quot; or create a more flexible formula able to give other point scores, not only 500, 1000, 1500... The score of each problem will be determined at the end of the competition.\u003c/p\u003e\u003cp\u003eAnother good feature of this is that with a bit for data mining rounds can be rated according to their quality. We can set expectations for a round and if a round does not meet the criteria it will be considered \u0026quot;not-good\u0026quot;. I personally dislike rounds where only 3 out of 5 problems get any solutions and I believe such rounds should be avoided. \u003c/p\u003e\u003c/div\u003e","tags":["scoring"]}}
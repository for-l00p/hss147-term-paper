{"status":"OK","result":{"originalLocale":"en","allowViewHistory":true,"creationTimeSeconds":1518444459,"rating":0,"authorHandle":"ilim","modificationTimeSeconds":1518445046,"id":57719,"title":"\u003cp\u003eMETU Mashup Volume #1 — Editorial\u003c/p\u003e","locale":"en","content":"\u003cdiv class\u003d\"ttypography\"\u003e\u003ch4\u003eA. Declined Finalists\u003c/h4\u003e\u003cp\u003eThere is not much to say about this problem. The basic idea is finding the maximum element in the array, say A\u003csub\u003emax\u003c/sub\u003e. If A\u003csub\u003emax\u003c/sub\u003e is greater than 25, then \u003cem\u003e(A\u003csub\u003emax\u003c/sub\u003e — 25)\u003c/em\u003e people would have to have declined. If A\u003csub\u003emax\u003c/sub\u003e is smaller than or equal to 25, then nobody has to decline for the contestant with the rank A\u003csub\u003emax\u003c/sub\u003e to participate in the finals.\u003c/p\u003e\u003ch4\u003eB. The Contest\u003c/h4\u003e\u003cp\u003eThe problem is a typical greedy one, where you have to keep track of the problems you solved and the time you are at. The crux of the idea behind the solution is that it is \u003cem\u003eoptimal\u003c/em\u003e to submit the problems in the non-decreasing order of time it takes to solve them. It is usually hard to give official proof of greedy solutions, so I will omit the proof, but you may try to figure it out on your own. (I\u0027d recommend proof by contradiction)\u003c/p\u003e\u003cp\u003eThen, you may just keep an \u003cem\u003eevent queue\u003c/em\u003e (i.e. a technique frequently used in computational geometry problems — commonly associated with plane/line sweep) of the starting and ending times of the submittable periods. You can keep the problem(s) you\u0027re working on (and the ones you have done) in some data structure( e.g. array, vector, set, etc.) and update it accordingly at each significant time. (i.e. when you enter or exit a period where you can submit a problem) Finally, you may record the times at which a submission is made, and return the last (or greatest) one of those.\u003c/p\u003e\u003ch4\u003eC. Dishonest Sellers\u003c/h4\u003e\u003cp\u003eThe idea behind the idea is again, greedy, in a way. You need to buy k of n elements right now. You should basically purchase all the elements current prices of which are cheaper than their prices after the discount. Let\u0027s say you bought all those \u003cem\u003eM\u003c/em\u003e elements that are more profitable to buy. If \u003ccode\u003ek \u0026gt; M\u003c/code\u003e, then you need to buy \u003ccode\u003e(k - M)\u003c/code\u003e more elements and have to suffer some losses. The strategy in buying those is trying to minimize your losses. To achieve that, you can basically sort the remaining items based on the difference between their current and post-discount prices, in non-decreasing order. Then just pick the first \u003ccode\u003e(k-M)\u003c/code\u003e elements, which you will buy now. The rest, you will purchase after the discount ends. The sum of the relevant prices based on what items you bought now yields the result.\u003c/p\u003e\u003ch4\u003eD. Dijkstra?\u003c/h4\u003e\u003cp\u003eThis is a typical question that asks you to implement \u003cem\u003eDijkstra\u0027s algorithm\u003c/em\u003e to find the shortest path to a vertex on a weighted, undirected graph. There are two modifications to the typical Dijkstra\u0027s algorithm implementation. The first is the fact that the graph is not simple, meaning that there may be multiple edges between any pair of nodes \u003cem\u003eu\u003c/em\u003e, \u003cem\u003ev\u003c/em\u003e. This is relatively easy to handle. My implementation basically filters the adjacency list right after reading the input, and keeps only the edge with the least weight between any nodes \u003cem\u003eu\u003c/em\u003e, \u003cem\u003ev\u003c/em\u003e.\u003c/p\u003e\u003cp\u003eThe second trick I used in my implementation was essential when I exceeded the memory limit in my first submission. Depending on your implementation, the priority queue used in Dijkstra\u0027s algorithm may swell, due to keeping multiple instances of a single vertex, only one of which is the current minimal route. It usually does not cause any issue in most problems where vertex and edge bounds are a little less, but apparently, a case in the tests is good enough to filter out bad implementations like mine. The trick I applied was using a \u003ccode\u003estd::set\u003c/code\u003e instead of a standard priority queue. For each node \u003cem\u003eu\u003c/em\u003e, whenever I found a cheaper route to \u003cem\u003eu\u003c/em\u003e, I took out the older \u003cem\u003eoptimal path\u003c/em\u003e inside that set if one existed. This causes a little more time burden on the algorithm. However, since insertion times of both the \u003ccode\u003estd::priority_queue\u003c/code\u003e and \u003ccode\u003estd::set\u003c/code\u003e are \u003cstrong\u003eO(logN)\u003c/strong\u003e, and erasing an element from std::set also takes \u003cstrong\u003eO(logN)\u003c/strong\u003e, the difference in performance was just a constant factor at each iteration. (**N** refers to the number of elements in the set)\u003c/p\u003e\u003ch4\u003eE. Lucky Pair\u003c/h4\u003e\u003cp\u003eTo be added soon.\u003c/p\u003e\u003ch4\u003eF. Test\u003c/h4\u003e\u003cp\u003eMy solution, unlike some others who used a \u003ca href\u003d\"https://www.geeksforgeeks.org/searching-for-patterns-set-2-kmp-algorithm/\"\u003eKMP\u003c/a\u003e-based idea, turns the problem into a graph problem. Basically if each string is to be considered as a node in a graph, and the length of the maximal intersection are the directed edges between them, then the problem is reduced to finding the longest path in that graph. The length of the longest path can then be reduced from the sum of the length of all three strings, giving the result.\u003c/p\u003e\u003cp\u003eThe challenge here was finding the maximal intersection of each pair of strings, which could naively be done in \u003cstrong\u003eO(N\u003csup\u003e2\u003c/sup\u003e)\u003c/strong\u003e. I believe it would be possible do achieve this task using a method similar to the prefix-suffix trick used on KMP, but not knowing KMP, I tried to modify the naive approach to achieve \u003cstrong\u003eO(NlogN)\u003c/strong\u003e complexity. Sadly, techniques like binary searching on the maximum length of common suffix-prefix pair does not work in a problem like this, because the interval is not monotonic. For instance, for \u003ccode\u003ek\u003d1\u003c/code\u003e and \u003ccode\u003ek\u003d3\u003c/code\u003e there may be an intersection, but for \u003ccode\u003ek\u003d2\u003c/code\u003e there may be no intersection.\u003c/p\u003e\u003cp\u003eAfter a long while, I thought of precomputing a frequency array(i.e. an array each cell of which corresponds to a letter, and denotes how many times it appears in a given string) for each possible suffix and prefix of each string. Then, I modified my naive implementation to compare the substrings \u003cem\u003eif only\u003c/em\u003e their frequency arrays are the same. Since the problem with the \u003cstrong\u003eO(N\u003csup\u003e2\u003c/sup\u003e)\u003c/strong\u003e approach was doing \u003cstrong\u003eO(N)\u003c/strong\u003e substring comparisons, each of which took \u003cstrong\u003eO(N)\u003c/strong\u003e, reducing the number of times that comparison was made helped improve the complexity, and I got an accepted. Note that it is not sufficient to compare \u003cem\u003ejust\u003c/em\u003e the frequency arrays to consider two strings to be equal. As you do not compare the order the elements appear in, that test alone may give false-positives, leading you to an incorrect result.\u003c/p\u003e\u003cp\u003eFinally, you also need to take into account the cases where one (or two) of the given strings being subsets of the third. The strategy is to get the minimum of the results one of which was yielded by solving the graph problem, and the other was obtained by checking the subset cases.\u003c/p\u003e\u003ch4\u003eG. Game of Stones\u003c/h4\u003e\u003cp\u003eThis problem is a modification of the Nim game, which is a rather popular game theory subject. To solve it, you need to familiarize yourself with finding the Grundy numbers of a game, and study how it is possible to combine the grundy numbers of multiple games to decide the winner. Instead of telling you in detail, let me provide you with some reading material that may help you internalize the intuition, and better understand the proofs regarding the idea. \u003ca href\u003d\"http://web.mit.edu/sp.268/www/nim.pdf\"\u003eImpartial games notes\u003c/a\u003e by MIT is a relatively concise resource on the topic. Also, GeeksForGeeks website \u003ca href\u003d\"https://www.geeksforgeeks.org/combinatorial-game-theory-set-3-grundy-numbersnimbers-and-mex/\"\u003eoffers\u003c/a\u003e a narrative as well as an implementation of the idea.\u003c/p\u003e\u003ch4\u003eH. Random Query\u003c/h4\u003e\u003cp\u003eExpected value problems feel always tricky to me. In this one, I spent a considerable time on figuring out a way to query the unique elements in each range. However, even my best candidate for an idea, which was a segment-tree based one, failed for some of the possible ranges. It dawned on me, although rather lately, that as opposed to counting the unique elements for each edge, the trick is to count the number of intervals for which a particular element is unique. Then, basically, that element will have contributed that many times to the expected value. Iterating over each value \u003cem\u003ev\u003c/em\u003e and keeping each occurrence of \u003cem\u003ev\u003c/em\u003e in somewhere is a good way to start. Since the magnitude of the values are capped at 10\u003csup\u003e6\u003c/sup\u003e, it is possible to achieve this in the given time.\u003c/p\u003e\u003cp\u003eOne thing to keep in mind is taking into account the intervals where there are multiple instances of a single value \u003cem\u003ev\u003c/em\u003e. Then, their combined contribution will be 1 for each such range. A neat way to find the number of intervals where there are multiple instances of the value \u003cem\u003ev\u003c/em\u003e is subtracting from N\u003csup\u003e2\u003c/sup\u003e, which is the total amount of valid intervals, the number of intervals where there aren\u0027t any \u003cem\u003ev\u003c/em\u003e values, and the number of intervals where \u003cem\u003ev\u003c/em\u003e is unique.\u003c/p\u003e\u003cp\u003eOnce the sum of each unique and combined contribution of each value is found, then that total may be divided to the number of available ranges N\u003csup\u003e2\u003c/sup\u003e, yielding the expected value.\u003c/p\u003e\u003ch4\u003eI. Level Generation\u003c/h4\u003e\u003cp\u003eBasically the problem asks to start with a \u003cem\u003eMinimum Spanning Tree\u003c/em\u003e, where each edge is a bridge, and wants us to try adding as many new edges as possible with the constraint that the number of bridges is greater than or equal to the number of non-bridges.\u003c/p\u003e\u003cp\u003eWorking on the sample cases, I initially thought that every new edge added to the graph would create a cycle, and every bridge included in that cyclic path would become non-bridges. So, a strategy with which it is optimal to add new edges could be by forming the most minimal cycles in the MST. Then, I noticed that, once you form a cycle C\u003csub\u003e1\u003c/sub\u003e, you need not switch to finding another one. You can keep adding new edges to C\u003csub\u003e1\u003c/sub\u003e until it becomes fully connected. Once C\u003csub\u003e1\u003c/sub\u003e becomes a fully connected graph with \u003ccode\u003e|V|\u003c/code\u003e vertices, you may then just introduce another vertex to your fully-connected graph, losing a single bridge, but gaining \u003ccode\u003e|V|\u003c/code\u003e possible edge additions. (i.e. until it becomes fully connected again)\u003c/p\u003e\u003cp\u003eIt is easier to internalize this strategy by considering a sample case. Consider \u003ccode\u003eN \u003d 10\u003c/code\u003e. Now, if we draw the most intuitive MST of an undirected graph with no weights, it would be a straight line. Then, introducing the shortest cycle is just connecting the last node with the 3\u003csup\u003erd\u003c/sup\u003e from the last. Now, the graph has 7 bridges and 3 non-bridge edges. The difference between those two is large enough to let us add some more non-bridge edges. So, since the last 3 nodes already form a fully-connected graph, we have to introduce the 4\u003csup\u003eth\u003c/sup\u003e from the last node into that graph. That way, we can add up to 2 more new edges, and the only bridge we lost was the edge between 3\u003csup\u003erd\u003c/sup\u003e node from the last and the 4\u003csup\u003eth\u003c/sup\u003e node from the last. Adding those edges, we end up with 6 bridges and 6 non-bridges. If we add any more edges, the bridges will not be the majority of the edges in the graph, so we need to stop there. So, for \u003ccode\u003eN \u003d 10\u003c/code\u003e, the answer is 12.\u003c/p\u003e\u003cp\u003eGiven that there are 10\u003csup\u003e5\u003c/sup\u003e queries, and the number of vertices may go up to 2.10\u003csup\u003e9\u003c/sup\u003e, it is essential to find the number of vertices \u003ccode\u003e|V|\u003c/code\u003e of the fully (or almost fully) connected graph in less than linear time. If we loop through possible \u003ccode\u003e|V|\u003c/code\u003e values in linear fashion, we will exceed the time limit. The idea I implemented was using binary search on possible values, which has the complexity of \u003cstrong\u003eO(logN)\u003c/strong\u003e. Once the range is reduced to less than 10, I halted the binary search and linearly went through the range to find the maximum number of edges it was possible to add. Since the length of the reduced range is negligible, the linear searching portion did not exceed the time limit.\u003c/p\u003e\u003ch4\u003eJ. Nagini\u003c/h4\u003e\u003cp\u003eTo be added soon.\u003c/p\u003e\u003c/div\u003e","tags":[]}}
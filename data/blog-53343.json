{"status":"OK","result":{"originalLocale":"en","allowViewHistory":true,"creationTimeSeconds":1500301365,"rating":-7,"authorHandle":"sdssudhu","modificationTimeSeconds":1500304781,"id":53343,"title":"\u003cp\u003eAn alternative trick to solve many HLD problems using basic dfs\u003c/p\u003e","locale":"en","content":"\u003cdiv class\u003d\"ttypography\"\u003e\u003cp\u003eI use this post to try and explain an alternative to HLD that I have been using for more than a couple of months.\u003c/p\u003e\u003cp\u003eAs many of you might be familiar HLD is a very tough thing to implement and is time-taking and must be implemented carefully.\u003c/p\u003e\u003cp\u003eAlso if HLD sums are asked in short contests then it is going to be difficult to implement. Hence I came up with this technique.\u003c/p\u003e\u003cp\u003eI will try and explain this concept with the \u003ca href\u003d\"https://www.codechef.com/JULY17/problems/PSHTTR\"\u003ejuly long contest problem\u003c/a\u003e.\u003c/p\u003e\u003cp\u003eIn this problem the value of number of nodes is given to be 10^5 which means maximum depth of the tree is 10^5 (worst case). What I do is I perform a dfs in the tree and for every node whose depth%1000\u003d1, I store the values of all its ancestors.\u003c/p\u003e\u003cp\u003eIn worst case the memory complexity is (1000+2000+...10^5)\u003d 1000(1+2+3+....+100) \u003d 5*10^6. After this I sort all these values and take a prefix xor.\u003c/p\u003e\u003cp\u003eNow for each query I have to travel up atmost 1000 ancestors and arrive at an ancestor whose depth%1000\u003d1 and from there we can find xor of all elements less than k. We do this for both nodes U and V(source and destination). Because of the property of XOR all the values of the ancestors are canceled out.\u003c/p\u003e\u003cp\u003eHence each query is (1000*Q) in the worst case.\u003c/p\u003e\u003cp\u003eThough this is somewhat testing the upper limits we can actually dynamically change the value in which we store the ancestors(1000 in this case). However this has not been required so far for me.\u003c/p\u003e\u003cp\u003eThis is because we such situations(which test upper limits) rarely occur but even for that depending on the tree we can change the depth value.\u003c/p\u003e\u003cp\u003eAnother question I solved using this techique is \u003ca href\u003d\"https://www.codechef.com/MAY17/problems/GPD\"\u003eGPD\u003c/a\u003e in codechef. GPD can also be solved using persistent trie but this method is far more easier.\u003c/p\u003e\u003cp\u003eMy solutions for : \u003ca href\u003d\"https://www.codechef.com/viewsolution/14469219\"\u003ePSHTTR\u003c/a\u003e  : \u003ca href\u003d\"https://www.codechef.com/viewsolution/13485588\"\u003eGPD\u003c/a\u003e\u003c/p\u003e\u003cp\u003eIn case for sum of values in the path between 2 nodes we can store sum of ancestors and we can find answer by:\u003c/p\u003e\u003cp\u003esum upto U + sum upto V â€” sum upto LCA(U,V)\u003c/p\u003e\u003cp\u003eUpd1: If the maximum depth of tree is less than 1000 you can directly climb up the tree and do the calculations.\u003c/p\u003e\u003cp\u003eAlso as \u003ca href\u003d\"//codeforces.com/profile/Diversity\"\u003eDiversity\u003c/a\u003e pointed out there can be cases where there are 10^5-1000 nodes which have depth%1000\u003d1. For overcoming this we can have an initial dfs that has a count of depth%1000\u003d1,2,3,4,...,999 and we can choose values which satisfy the memory limit using count(This is the dynamic change of value I mentioned).Also I believe we can strech upto depth%2000. \u003c/p\u003e\u003cp\u003eNote: I have not tried implementing this technique of changing node depth of storing values dynamically.\u003c/p\u003e\u003c/div\u003e","tags":["hld","#graph","tutorial"]}}
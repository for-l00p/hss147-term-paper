{"status":"OK","result":{"originalLocale":"en","allowViewHistory":false,"creationTimeSeconds":1519796258,"rating":3,"authorHandle":"daihan","modificationTimeSeconds":1519796462,"id":58080,"title":"\u003cp\u003eWhat is the time complexity of Euclidian algorithm .\u003c/p\u003e","locale":"en","content":"\u003cdiv class\u003d\"ttypography\"\u003e\u003cp\u003eI searched google , about this , I understand only that worst case happens when a,b are consecutive fibonacci numbers .\u003c/p\u003e\u003cp\u003eSomeone says Euclidian algo\u0027s time complexity is O(log(a+b) ) , some says O(log(n) ) . But their explanation is not clear to me . I read the first answer from here : \u003ca href\u003d\"https://stackoverflow.com/questions/3980416/time-complexity-of-euclids-algorithm\"\u003ehttps://stackoverflow.com/questions/3980416/time-complexity-of-euclids-algorithm\u003c/a\u003e .\u003c/p\u003e\u003cp\u003eI pasted it bellow . It describes the analysis of euclid algo . \u003c/p\u003e \u003cpre\u003e\u003ccode\u003e\u0026quot;One trick for analyzing the time complexity of Euclid\u0027s algorithm is to follow what happens over two iterations:\n\na\u0027, b\u0027 :\u003d a % b, b % (a % b)\nNow a and b will both decrease, instead of only one, which makes the analysis easier. You can divide it into cases:\n\nTiny A: 2a \u0026lt;\u003d b\nTiny B: 2b \u0026lt;\u003d a\nSmall A: 2a \u0026gt; b but a \u0026lt; b\nSmall B: 2b \u0026gt; a but b \u0026lt; a\nEqual: a \u003d\u003d b\nNow we\u0027ll show that every single case decreases the total a+b by at least a quarter:\n\nTiny A: b % (a % b) \u0026lt; a and 2a \u0026lt;\u003d b, so b is decreased by at least half, so a+b decreased by at least 25%\nTiny B: a % b \u0026lt; b and 2b \u0026lt;\u003d a, so a is decreased by at least half, so a+b decreased by at least 25%\nSmall A: b will become b-a, which is less than b/2, decreasing a+b by at least 25%.\nSmall B: a will become a-b, which is less than a/2, decreasing a+b by at least 25%.\nEqual: a+b drops to 0, which is obviously decreasing a+b by at least 25%.\nTherefore, by case analysis, every double-step decreases a+b by at least 25%. There\u0027s a maximum number of times this can happen before a+b is forced to drop below 1. The total number of steps (S) until we hit 0 must satisfy (4/3)^S \u0026lt;\u003d A+B. Now just work it:\n\n(4/3)^S \u0026lt;\u003d A+B\nS \u0026lt;\u003d lg[4/3](A+B)\nS is O(lg[4/3](A+B))\nS is O(lg(A+B))\nS is O(lg(A*B)) //because A*B asymptotically greater than A+B\nS is O(lg(A)+lg(B))\n//Input size N is lg(A) + lg(B)\nS is O(N)\nSo the number of iterations is linear in the number of input digits. For numbers that fit into cpu registers, it\u0027s reasonable to model the iterations as taking constant time and pretend that the total running time of the gcd is linear.\n\nOf course, if you\u0027re dealing with big integers, you must account for the fact that the modulus operations within each iteration don\u0027t have a constant cost. Roughly speaking, the total asymptotic runtime is going to be n^2 times a polylogarithmic factor. Something like n^2 lg(n) 2^O(log* n). The polylogarithmic factor can be avoided by instead using a binary gcd.\u0026quot;\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003eIn the upper statement , I am unable to understand this line (4/3)^S \u0026lt;\u003d A+B . How does it come from ? If anyone can explain please help . \u003c/p\u003e\u003cp\u003e\u003cstrong\u003eIf anyone has other easier explanation , please share .\u003c/strong\u003e\u003c/p\u003e\u003c/div\u003e","tags":["help"]}}
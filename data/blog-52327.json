{"status":"OK","result":{"originalLocale":"en","allowViewHistory":true,"creationTimeSeconds":1496364743,"rating":87,"authorHandle":"ItsNear","modificationTimeSeconds":1496385571,"id":52327,"title":"\u003cp\u003eUsing recurrent neural networks to predict next tokens in the java solutions\u003c/p\u003e","locale":"en","content":"\u003cdiv class\u003d\"ttypography\"\u003e\u003cp\u003eHi everyone,\u003c/p\u003e\u003cp\u003eToday we are going to show few demos of Sequence to Sequence models for code completion.\u003c/p\u003e\u003cp\u003eBut before diving into details, we, as usual, want to ask you to participate in collecting data for the future models in our labeling platform here:\u003c/p\u003e\u003cp\u003e\u003ca href\u003d\"https://r-nn.com\"\u003ehttps://r-nn.com\u003c/a\u003e\u003c/p\u003e\u003cp\u003eThere are real money rewards associated with all the tasks in the platform.\u003c/p\u003e\u003ch1\u003eThe Demo\u003c/h1\u003e\u003cp\u003eIn the first demo we trained a seq2seq model, more on which below, on all the Accepted java solutions on CodeForces. The goal of the model is to predict the next token based on all the tokens seen so far. We then plug the output of the model into a code editor (the arena on the video is an unfinished project I will write about separately) to see how it behaves when one actually solves a competitive programming problem. Here we are solving problem A from Codeforces Round 407:\u003c/p\u003e\u003cp\u003e\u003ca href\u003d\"https://youtu.be/icoAK6yMCjg\"\u003ehttps://youtu.be/icoAK6yMCjg\u003c/a\u003e\u003c/p\u003e\u003cp\u003eNotice how early on the model nearly perfectly predicts all the tokens, which is not surprising, since most of the Java solutions begin with rather standard imports, and the solution class definition. Later it perfectly predicts the entire line that reads \u003ccode\u003en\u003c/code\u003e, but doesn’t do that well predicting reading \u003ccode\u003ek\u003c/code\u003e, which is not surprising, since it is a rather rare name for the second variable to be read.\u003c/p\u003e\u003cp\u003eThere are several interesting moments in the video. First, note how after \u003ccode\u003eint n \u003d\u003c/code\u003e it predicts \u003ccode\u003esc\u003c/code\u003e, understanding that \u003ccode\u003en\u003c/code\u003e will probably be read from the scanner (and while not shown in the video, if the scanner name was \u003ccode\u003ein\u003c/code\u003e, the model would have properly predicted \u003ccode\u003ein\u003c/code\u003e after \u003ccode\u003eint n \u003d\u003c/code\u003e), however when the line starts with \u003ccode\u003eint ans \u003d\u003c/code\u003e, it then properly predicts \u003ccode\u003e0\u003c/code\u003e, since \u003ccode\u003eans\u003c/code\u003e is rarely read from the input.\u003c/p\u003e\u003cp\u003eThe second interesting moment is what happens when we are printing the answer. At first when the line contains \u003ccode\u003eSystem.out.println(ans\u003c/code\u003e it predicts a semicolon (mistakenly) and the closing parenthesis as possible next tokens, but not \u003ccode\u003e- 1\u003c/code\u003e, however when we introduce the second parenthesis \u003ccode\u003eSystem.out.println((ans\u003c/code\u003e, it then properly predicts \u003ccode\u003e-1\u003c/code\u003e, closing parenthesis, and the division by two.\u003c/p\u003e\u003cp\u003eYou can also notice a noticeable pause before the \u003ccode\u003efor\u003c/code\u003e loop is written. This is due to the fact that using such artificial intelligence suggestions completely turns off the natural intelligence the operator of the machine possesses :)\u003c/p\u003e\u003cp\u003eOne concern with such autocomplete is that in the majority of cases most of the tokens are faster to type than to select from the list. To address it, in the second demo we introduce beam search that searches for the most likely sequences of tokens. Here’s what it looks like:\u003c/p\u003e\u003cp\u003e\u003ca href\u003d\"https://youtu.be/dNQV05Qry3A\"\u003ehttps://youtu.be/dNQV05Qry3A\u003c/a\u003e\u003c/p\u003e\u003cp\u003eHere there are more rough edges, but notice how early on the model can correctly predict entire lines of code.\u003c/p\u003e\u003cp\u003eCurrently we do not condition the predictions on the task. Partially because number of tasks available on the Internet is too small for a machine learning model to predict anything reasonable (so, please help us fix it by participating here: \u003ca href\u003d\"https://r-nn.com\"\u003e\u003c/a\u003e\u003ca\u003ehttps://r-nn.com\u003c/a\u003e). Once we have a working model that is conditioned on the statement, we expect it to be able to predict variable names, snippets to read and write data and computing some basic logic.\u003c/p\u003e\u003cp\u003eLet’s review Seq2Seq models in the next section.\u003c/p\u003e\u003ch1\u003eSequence to sequence models\u003c/h1\u003e\u003cp\u003eSequence to Sequence (Seq2Seq for short) has been a ground-breaking architecture in Deep Learning. Originally published by \u003ca href\u003d\"https://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf\"\u003eSutskever, Vinyals and Le\u003c/a\u003e, this family of models achieved state-of-the-art results in Machine Translation, Parsing, Summarization, Text to Speech and other applications.\u003c/p\u003e\u003cp\u003eGenerally Seq2Seq is an extension of regular Recurrent Neural Networks (we have mentioned them before in \u003ca href\u003d\"//codeforces.com/blog/entry/52305\"\u003e\u003c/a\u003e\u003ca\u003ehttp://codeforces.com/blog/entry/52305\u003c/a\u003e), where model first encodes input tokens and then tries to produce set of output tokens by decoding on token at a time.\u003c/p\u003e\u003cp\u003e\u003cimg src\u003d\"/predownloaded/f3/4d/f34dc9314c29c0742754edf540f5070288707903.png\" style\u003d\"max-width: 100.0%;max-height: 100.0%;\" /\u003e\u003c/p\u003e\u003cp\u003eFor example on this picture the model is first fed three inputs \u003ccode\u003eA\u003c/code\u003e, \u003ccode\u003eB\u003c/code\u003e and \u003ccode\u003eC\u003c/code\u003e and then it is given \u003ccode\u003e\u0026lt;EOS\u0026gt;\u003c/code\u003e token to indicate that it should start predicting output sequence [W, X, Y, Z]. Here each token is represented as a continuous vector [embedding] that is learned jointly with parameters of the RNN model.\u003c/p\u003e\u003cp\u003eAt training time, model is fed with correct output tokens shifted by one, so it can learn dependencies in the data. The signal comes from maximizing the log probability of a correct output given the source sequence.\u003c/p\u003e\u003cp\u003eOnce training is complete this model can be used to produce most probable output sequences. Usually to search for most likely decodings a simple left-to-right beam search is applied. Beam search is a greedy algorithm of keeping a small number B of partial hypotheses to find the most likely decoding. At each step we extend these hypotheses by every possible token in the vocabulary and then discard all but B most likely hypotheses according to model’s probability. As soon as “” (End of Sequence) symbol is appended to hypothesis, it is removed from the beam and is added to a set of complete hypotheses.\u003c/p\u003e\u003cp\u003eThe model that we use right now is not a seq2seq per se, since there’s no input sequence given to it, so it only consists of the decoder part of the seq2seq. After being trained on the entire set of CodeForces java solutions, it achieved 68% accuracy of predicting the next token correctly, and 86% of predicting it among top 5 predictions. If the first character of the token is already known, the prediction reaches 94%. It is unlikely that a vanilla seq2seq will achieve a performance sufficient to be usable in practice, but it establishes a good baseline to compare the future models against.\u003c/p\u003e\u003c/div\u003e","tags":["neural networks","seq2seq","nobody reads tags"]}}
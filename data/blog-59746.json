{"status":"OK","result":{"originalLocale":"en","allowViewHistory":true,"creationTimeSeconds":1527621916,"rating":336,"authorHandle":"ItsNear","modificationTimeSeconds":1527703968,"id":59746,"title":"\u003cp\u003eAre we close to machines solving ICPC problems?\u003c/p\u003e","locale":"en","content":"\u003cdiv class\u003d\"ttypography\"\u003e\u003cp\u003eHi, all,\u003c/p\u003e\u003cp\u003eI with few other folks at \u003ca href\u003d\"http://near.ai/blog\"\u003eNEAR\u003c/a\u003e work on teaching machines to program. A particularly exciting sub-project of that is teaching machines to solve competitive programming problems.\u003c/p\u003e\u003cp\u003eIn this post I would like to give a quick overview of where the state of the art is today, what the major challenges are, why this is not a popular area of research, and how the CodeForces community can help to address some of the issues the program synthesis community is facing today.\u003c/p\u003e\u003cp\u003eWe also have a certain budged allocated for this project, and we are paying to the CodeForces members who help us with some data annotation challenges. We have paid more than $10k in our first two annotation projects, and are launching three more projects today. Scroll to the end if you are interested.\u003c/p\u003e\u003ch1\u003eCompetitive programming as a benchmark\u003c/h1\u003e\u003cp\u003eWith the emergence of deep learning, neural networks started performing almost at a human level in many tasks: visual object recognition, translation between languages, speech recognition, and many other. One area where they haven\u0027t shown anything exciting yet is programming. I will talk about state of the art below, but overall neural networks are nowhere close today to doing actual coding.\u003c/p\u003e\u003cp\u003eBut what would it even mean to solve programming? What would be a good benchmark and milestones to recognize? One such milestone is solving competitive programming. We are not talking here about building a bot that would perform at a \u003ca class\u003d\"rated-user user-legendary\" href\u003d\"/profile/tourist\" title\u003d\"Legendary grandmaster tourist\"\u003e\u003cspan class\u003d\"legendary-user-first-letter\"\u003et\u003c/span\u003eourist\u003c/a\u003e level, but at least solving A and B Division two would be super impressive. Even this is actually very hard for computers, and I will talk about why below in the Open Challenges section.\u003c/p\u003e\u003ch1\u003eState of the art\u003c/h1\u003e\u003cp\u003eThe area of research that is tasked with automated program generation is called Program Synthesis. The two most common sub-areas of it are programming from examples (synthesizing a program from few input/output examples of what it should do) and programming from description (synthesizing a program from an English description).\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eProgramming from examples\u003c/strong\u003e has a long history. One known example that you can run in your browser is \u003ca href\u003d\"http://nautilus.cs.miyazaki-u.ac.jp/~skata/MagicHaskeller.html\"\u003eMagic Haskeller\u003c/a\u003e, which is a very sophisticated system that synthesizes rather nontrivial Haskell programs from just \u003cstrong\u003eone\u003c/strong\u003e example.\u003c/p\u003e\u003cp\u003eThere were some applications of program synthesis from example in commercial products. Modern distributions of Excel come with a feature called FlashFill. If you have a column of names, and a column of last names, and enter an email in the third column in a form of \u0026quot;j.doe@gmail.com\u0026quot;, FlashFill will immediately synthesize a program that concatenates the first letter of the first name with a dot, the last name and the \u0026quot;@gmail.com\u0026quot; and suggest to auto-fill the rest of the column. This looks like magic, and took more than a year of work for a very strong team of program synthesis experts at Microsoft Research to deliver it.\u003c/p\u003e\u003cp\u003e\u003cimg alt\u003d\" \" src\u003d\"/predownloaded/44/70/447031158a663630d942387812fafbf0951c4d6d.jpg\" style\u003d\"max-width: 100.0%;max-height: 100.0%;\" /\u003e\u003c/p\u003e\u003cp\u003eUltimately the lab that created FlashFill created a general purpose framework \u003ca href\u003d\"https://microsoft.github.io/prose/\"\u003ePROSE\u003c/a\u003e in which one can create services like FlashFill by describing a domain-specific language and writing few C# functions.\u003c/p\u003e\u003cp\u003eA separate epic paper from a different lab at MSR called \u003ca href\u003d\"https://openreview.net/pdf?id\u003dSk6W5JJke\"\u003eTerpreT\u003c/a\u003e serves as a great review and comparison of different approaches to programming from examples. It compares using SMT solvers, linear programming and neural networks, and shows that as of today using more traditional approaches such as SMT solvers still noticeably outperforms deep learning approaches.\u003c/p\u003e\u003cp\u003eAn early attempt to apply programming by example to solving competitive programming is a paper from the same lab called \u003ca href\u003d\"https://openreview.net/pdf?id\u003dByldLrqlx\"\u003eDeep Coder\u003c/a\u003e. It can synthesize short programs for very simple problems by seeing just a few examples, and uses a combination of both deep learning and several traditional approaches.\u003c/p\u003e\u003cp\u003e\u003cimg alt\u003d\" \" src\u003d\"/predownloaded/1e/a2/1ea2939c6fd1090f95f56d9054468523d52529f3.jpeg\" style\u003d\"max-width: 100.0%;max-height: 100.0%;\" /\u003e\u003c/p\u003e\u003cp\u003eAnother great inspiring example of applying program synthesis from examples is a paper called \u003ca href\u003d\"https://people.eecs.berkeley.edu/~mangpo/www/papers/chlorophyll-pldi14.pdf\"\u003eChlorophyll\u003c/a\u003e, in which the technique is applied to optimizing assembly programs. It does it in the following way:\u003c/p\u003e \u003col\u003e   \u003cli\u003eGiven a short program that needs to be optimized and a few tests (possibly zero), synthesize the shortest program that is consistent with the existing tests.\u003c/li\u003e   \u003cli\u003eUsing an SMT solver, try to prove that the synthesized and the given programs produce the same output on all possible tests. If proven, the synthesized program is an optimized version of the given program. If disproved, the SMT solver would find a test on which the two programs produce different output. Add the test to the set of tests and go to (1).\u003c/li\u003e \u003c/ol\u003e\u003cp\u003eMindblowingly, not only this works, it generally converges for short programs in under 10 iterations!\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eProgramming from description\u003c/strong\u003e is a more recent area of research. Until the emergence of Deep Learning few years ago there was no sufficiently general way of extracting information from natural language. Arguably, even with the modern Deep Learning approaches natural language understanding is still at a pretty early stages, which is one of the major limitations.\u003c/p\u003e\u003cp\u003eThere were papers that attempt to solve synthesis of \u003ca href\u003d\"https://arxiv.org/pdf/1709.00103.pdf\"\u003eSQL\u003c/a\u003e, \u003ca href\u003d\"https://homes.cs.washington.edu/~mernst/pubs/nl-command-tr170301.pdf\"\u003eBASH\u003c/a\u003e and Java from description, but they all face the challenges described below. To my best knowledge no application of synthesis from description appears in any commercial product today.\u003c/p\u003e\u003ch1\u003eWho works on Program Synthesis?\u003c/h1\u003e\u003cp\u003eNumber of labs that work on program synthesis is actually rather small. Besides NEAR, here are several well-known ones:\u003c/p\u003e\u003cp\u003e\u003ca href\u003d\"https://sunblaze-ucb.github.io/program-synthesis/index.html\"\u003eBerkeley lab lad by Dawn Song\u003c/a\u003e\u003c/p\u003e\u003cp\u003eMIT lab lad by \u003ca href\u003d\"https://people.csail.mit.edu/asolar/\"\u003eArmando Solar-Lezama\u003c/a\u003e\u003c/p\u003e\u003cp\u003eETH Zurich lab (notably \u003ca href\u003d\"https://www.sri.inf.ethz.ch/raychev.php\"\u003eVeselin Raychev\u003c/a\u003e and \u003ca href\u003d\"https://www.sri.inf.ethz.ch/vechev.php\"\u003eMartin Vechev\u003c/a\u003e), which ultimately incorporated into \u003ca href\u003d\"https://www.deepcode.ai/\"\u003eDeepCode.ai\u003c/a\u003e\u003c/p\u003e\u003cp\u003eMicrosoft Research has a long history of working on program synthesis. Here are two good publication aggregates: \u003ca href\u003d\"https://www.microsoft.com/en-us/research/project/program-synthesis/#!publications\"\u003eone\u003c/a\u003e and \u003ca href\u003d\"https://www.microsoft.com/en-us/research/project/neural-program-synthesis/#!publications\"\u003etwo\u003c/a\u003e.\u003c/p\u003e\u003ch1\u003eOpen Challenges\u003c/h1\u003e\u003cp\u003eWhen we talk about programming from description, there are several large challenges, all remaining mostly unsolved as of today.\u003c/p\u003e\u003ch2\u003e1. Lack of data\u003c/h2\u003e\u003cp\u003eThis might sound crazy -- GitHub has so much code that even just crawling all of it is a non-trivial task, how could the data be lacking? But as a matter of fact, the data that is readily available, such as GitHub or StackOverflow, is not immediately usable because it is very noisy. Here are some datasets that we considered, and what challenges they come with:\u003c/p\u003e \u003col\u003e   \u003cli\u003e\u003cp\u003eSmall commits that fix a simple bug, and the text of the associated issue. The person who fixes a bug has a lot of context about how the code operates. This context varies drastically between projects, and without it even an experienced human expert would not be able to predict the change in the code given the issue text.\u003c/p\u003e\u003c/li\u003e   \u003cli\u003e\u003cp\u003eCommit messages and the code. Besides commits also depending significantly on the context (i.e. a human expert often won\u0027t be able to predict the code in the commit given a well-formed commit message), this dataset also has a lot of noise. People often provide incorrect commit messages, squash multiple features and fixes into one commit describing only one feature, or generally writing something like \u0026quot;Fixing a bug we found yesterday\u0026quot;.\u003c/p\u003e\u003c/li\u003e   \u003cli\u003e\u003cp\u003eDocstrings and function bodies. This one was very promising, but has two issues: a) docstring describes how a function shall be used, not how it is implemented, and predicting code from the usage pattern is a harder task than predicting code from a task description; and b) even in very good codebases quality docstrings are usually only provided for the public APIs, and the internal code is generally way less documented.\u003c/p\u003e\u003c/li\u003e \u003c/ol\u003e\u003cp\u003eOn the other hand, there\u0027s also competitive programming archives. Competitive programming as a dataset has several advantages:\u003c/p\u003e \u003col\u003e   \u003cli\u003e\u003cp\u003eThe code is short and self-contained.\u003c/p\u003e\u003c/li\u003e   \u003cli\u003e\u003cp\u003eNo context besides the problem statement is needed to write it.\u003c/p\u003e\u003c/li\u003e   \u003cli\u003e\u003cp\u003eThe dataset of statements and accepted solutions is very clean -- accepted solutions almost certainly solve the problem from the statement.\u003c/p\u003e\u003c/li\u003e \u003c/ol\u003e\u003cp\u003eThere are however some challenges:\u003c/p\u003e \u003col\u003e   \u003cli\u003e\u003cp\u003eWhile the number of solutions is very high (CodeForces alone has dozens of millions), the number of different problems is somewhat low. Our estimate suggests that the total number of different competitive programming problems in existence is somewhere between 200k and 500k, with ~50k being attainable with solutions when a reasonable effort is made. Only 1/3 of those problems are easy problems, so we are looking at having ~17k problems available for training.\u003c/p\u003e\u003c/li\u003e   \u003cli\u003e\u003cp\u003eThe statements contain a lot of fluff. Alice and Bob walking on a street and finding a palindrome laying around is something I had in my nightmares on multiple occasions. This unnecessary fluff paired with relatively low number of problem statements further complicates training the models.\u003c/p\u003e\u003c/li\u003e \u003c/ol\u003e\u003cp\u003eThe lack of data is also the primary reason why program synthesis is not a popular area of research, since researchers prefer to work on existing datasets rather than figuring out where to get data from. One anecdotal evidence is the fact that after MetaMind published the \u003ca href\u003d\"https://arxiv.org/pdf/1709.00103.pdf\"\u003eWikiSQL dataset\u003c/a\u003e, multiple papers were immediately submitted to the coming conferences, creating a minor spike in program synthesis popularity, despite the fact that the quality of the dataset is very low.\u003c/p\u003e\u003cp\u003eHaving said that, annotating the competitive programming dataset to a usable state would increase the popularity of the field, bringing closer the moment when we are competing against bots, and not just humans.\u003c/p\u003e\u003ch2\u003e2. External Knowledge\u003c/h2\u003e\u003cp\u003eConsider the following problem that was recently given on CodeForces: given an array of numbers of size 5000, how many triplets of numbers are there in the array such that the XOR of the three numbers is zero, and the three numbers can be the sides of a non-degenerate triangle.\u003c/p\u003e\u003cp\u003eThis is Div1 A/B level, with most people with experience solving it easily. However from a machine perspective it is extremely hard. Consider the following challenges:\u003c/p\u003e \u003col\u003e   \u003cli\u003e\u003cp\u003eThe model would need to realize that it doesn\u0027t need to fix all three numbers, it is sufficient to fix two numbers, and check if their XOR exists in the array. (this is besides the fact that the model would need to understand that fixing three numbers would not fit into timelimit)\u003c/p\u003e\u003c/li\u003e   \u003cli\u003e\u003cp\u003eThe model would need to somehow know what does it mean for three numbers to be sides of a non-degenerate triangle.\u003c/p\u003e\u003c/li\u003e \u003c/ol\u003e\u003cp\u003eOut of 50k problems that we have collected, only few had used that particular property of XOR in a similar way, and only a few dozen were in some way referring to the property of the sides of the triangle. To make a machine learning model be able to solve such a problem, one of the three things will need to happen:\u003c/p\u003e \u003col\u003e   \u003cli\u003e\u003cp\u003eWe will find a way to learn from few examples. One-shot learning is a hot topic of research, with some recent breakthroughs, but no existing approach would be immediately applicable here.\u003c/p\u003e\u003c/li\u003e   \u003cli\u003e\u003cp\u003eWe will find a way to provide the model with some external knowledge. This is an area that hasn\u0027t been researched much, and even figuring out how that knowledge shall be represented is a challenging research task.\u003c/p\u003e\u003c/li\u003e   \u003cli\u003e\u003cp\u003eWe will find a way to augment the dataset in such a way that a single concept that occurs only a few times in the dataset instead appears in a variety of contexts.\u003c/p\u003e\u003c/li\u003e \u003c/ol\u003e\u003cp\u003eAll three of those are open challenges. Until they are solved, we can consider an easier task: solving a competitive programming problem that is (either initially, or by the means of rewriting) very close to the solution, e.g:\u003c/p\u003e\u003cp\u003e\u0026quot;Given an array of numbers, are there two numbers \u003ccode\u003ea\u003c/code\u003e and \u003ccode\u003eb\u003c/code\u003e in that array such that \u003ccode\u003ec\u003c/code\u003e \u003d \u003ccode\u003ea\u003c/code\u003e XOR \u003ccode\u003eb\u003c/code\u003e also appears in the array, and the largest number among \u003ccode\u003ea\u003c/code\u003e, \u003ccode\u003eb\u003c/code\u003e and \u003ccode\u003ec\u003c/code\u003e is smaller than the sum of the remaining two?\u0026quot;\u003c/p\u003e\u003cp\u003eCan we solve it? With the modern technology not yet, and it makes sense to learn how to solve such problems before we even get to the more complex one-shot learning part.\u003c/p\u003e\u003ch2\u003e3. Inherent uncertainty in Deep Learning models\u003c/h2\u003e\u003cp\u003eDeep Learning models by design are probabilistic. Each prediction they make has some certainty associated with it, and therefore they generally keep having some chance of making a mistake even when trained well. If a model predicts code one token at a time, and a code snipped comprises 200 tokens, the model with 99% per-token accuracy would mess up at least one token with probability 87%. In natural language translation this is acceptable, but in Program Synthesis messing up one token most likely leads to a completely wrong program.\u003c/p\u003e\u003cp\u003eNotably, more traditional approaches from code synthesis (used primarily for programming by examples) don\u0027t suffer from the same problem. Marrying classic programming languages approaches with deep learning approaches is very desirable, but at this time very little success was achieved.\u003c/p\u003e\u003ch1\u003eIs it NEAR?\u003c/h1\u003e\u003cp\u003eAt NEAR, we are attempting to address many of the above problems. Here I will shortly cover our efforts that are relevant to solving competitive programming.\u003c/p\u003e\u003ch2\u003eData\u003c/h2\u003e\u003cp\u003eAs I mentioned above, even though competitive programming data is rather clean, it has some unnecessary fluff in the statements that we\u0027d love to get rid of. For that we asked the community to rewrite problem statements in such a way that only a very formal and dry description of what exactly needs to be done is left.\u003c/p\u003e\u003cp\u003eWe also used help of the community to collect a dataset of statement-solution pairs in which no external knowledge (such as properties of XOR or triangles) is needed to write the solution given the statement. We plan to release a large part of this dataset during \u003ca href\u003d\"https://uclmr.github.io/nampi/\"\u003eNAMPI 2018\u003c/a\u003e, so stay tuned.\u003c/p\u003e\u003cp\u003eThis dataset is a set of problems with statement super close to the solution. Such problems are easier for a machine than even the simplest real problems on a real contest, but this dataset not only serves as the first milestone (being able to solve it is a prerequisite to solving more complex problems), it is also a good curriculum learning step -- a model that was taught to solve such problems is expected to pick up solving more complex problems more easily than a model that was not.\u003c/p\u003e\u003ch2\u003eData augmentation\u003c/h2\u003e\u003cp\u003eTo augment the dataset above, we created an algorithm that gets a solution as an input, and produces a very low level statement (very close to the solution) as an output. The generator is built in such a way that the statements generated are close in terms of certain metrics to what people have written. Training on such generated dataset achieves a non-zero accuracy on the human generated dataset, and the closer we get the generated statements to what people write in terms of the measurable metrics (such as \u003ca href\u003d\"https://en.wikipedia.org/wiki/BLEU\"\u003eBLEU score\u003c/a\u003e), the better the accuracy of the trained model on the human-generated set is.\u003c/p\u003e\u003cp\u003e\u0026quot;Non-zero accuracy\u0026quot; might not sound very impressive, but if you consider the fact that it effectively means that the model manages to learn how to solve some (albeit very simple) human-generated problems by only being trained on a synthetic data, it is actually very promising, and is a huge step towards solving more complex problems, and ultimately solving actual contest problems on a live competition.\u003c/p\u003e\u003ch2\u003eFixing the uncertainty of Deep Learning models\u003c/h2\u003e\u003cp\u003eAt the core of our approaches are deep learning models that read in text and produce code as either a sequence of tokens, or an AST tree. Either way, as mentioned above, even a very well trained model has a very high chance of messing up at least one token.\u003c/p\u003e\u003cp\u003eTrying to address this problem is on its own very close to what competitive programmers do. We explore quite a few approaches. We presented one such approach in \u003ca href\u003d\"https://arxiv.org/pdf/1802.04335.pdf\"\u003eour ICLR workshop paper\u003c/a\u003e -- the idea is to perform a beam search on AST trees until we find an AST of a program that passes sample tests. By the time we were publishing that paper we didn\u0027t have a good human-annotated dataset yet, so all the results were presented on a semi-synthetic dataset, but the model itself was not changed much since then, and we still use it on the human-annotated data today.\u003c/p\u003e\u003cp\u003e\u003cimg alt\u003d\" \" src\u003d\"/predownloaded/6e/97/6e972e35c3345d9e4c1a32997f16abdc1654ad08.png\" style\u003d\"max-width: 100.0%;max-height: 100.0%;\" /\u003e\u003c/p\u003e\u003cp\u003eRunning beam-search in the tree space is pretty slow and memory-consuming, and requires some tricky optimizations, such as using persistent trees, precomputing results on the go and others. Anyone knows a good place where I can find people who are good at writing complex algorithms on trees and who\u0027d like to join a Silicon Valley-based VC-funded company that teaches machines to write code?\u003c/p\u003e\u003cp\u003eAnother approach we researched is letting another (or the same) deep learning model to fix the program if the first program that was generated doesn\u0027t pass the samples. The high level idea is to train a model that takes as an input a problem statement and a program from the previous iteration alongside with some feedback on why the program is wrong, and generates a new program (either from scratch, or by producing a diff). We have a publication that reports some early results that was also accepted as a \u003ca href\u003d\"https://openreview.net/pdf?id\u003dB1iZRFkwz\"\u003eworkshop paper at ICLR 2018\u003c/a\u003e.\u003c/p\u003e\u003cp\u003eTo get to some more interesting ideas, let\u0027s consider us asking the model to implement a binary search. Imagine that it mistakenly swaps lines \u003ccode\u003eleft \u003d middle\u003c/code\u003e and \u003ccode\u003eright \u003d middle\u003c/code\u003e. If you then feed the code back to the model, spotting this bug would be extremely hard. Even for humans it is a non-trivial task. So is the original source code the best medium to feed back to the model? One option we have been exploring is feeding an execution trace instead. If the model made the mistake above, the execution trace would always converge to one of the ends of the range instead of some value in the middle, which would provide way more information to the model than just the code itself. The challenge, however, is that execution traces of real programs are rather long, and condensing them to only show information that is likely to be relevant is an open and interesting challenge.\u003c/p\u003e\u003ch1\u003eHow can community help\u003c/h1\u003e\u003cp\u003eIf you find the concept interesting, and would like to help, there are (at least) two ways you can do it:\u003c/p\u003e\u003ch2\u003e1. Help us annotate more data (and get paid for that!)\u003c/h2\u003e\u003cp\u003eWe run a labeling platform at\u003c/p\u003e\u003cp\u003e\u003ca href\u003d\"https://r-nn.com\"\u003ehttps://r-nn.com\u003c/a\u003e\u003c/p\u003e\u003cp\u003eWhere we ask people to help us rewrite statements or annotate solutions. We have paid out more than $10k in the last few months, and are launching three new tasks there today. The invitation code to register is NEAR.\u003c/p\u003e\u003cp\u003eYou need to have sufficient written English skill to write statements that are understandable by English speakers to participate.\u003c/p\u003e\u003ch2\u003e2. Help us get more solutions\u003c/h2\u003e\u003cp\u003eIf you were solving problems on informatics.mccme.ru, and would be willing to share your account with me to crawl it, that would provide a lot of value. The website has great intro level problems, and presently we have no solutions for them.\u003c/p\u003e\u003cp\u003eWe also look for accounts on Timus, SPOJ, acmp.ru and main.edu.pl / szkopul.edu.pl.\u003c/p\u003e\u003c/div\u003e","tags":["near","program synthesis","code synthesis","nobody reads tags"]}}